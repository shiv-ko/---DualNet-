{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要なライブラリのインポート\n",
    "import numpy as np\n",
    "from tensorflow.keras import Input,Model\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,MaxPooling2D,concatenate,Conv2DTranspose,Add,Dense,Dropout,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import Precision, Recall,Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report,precision_score,roc_curve,f1_score,roc_auc_score,auc,confusion_matrix,classification_report\n",
    "from keras.optimizers import Adam\n",
    "#ファイルの呼び出し\n",
    "a=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/No Finding_array_5000.npy')\n",
    "a=a[:2500]\n",
    "a1=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/Atelectasis_array.npy')\n",
    "a2=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/Cardiomegaly_array.npy')\n",
    "a3=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/Effusion_array.npy')\n",
    "a4=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/Infiltration_array.npy')\n",
    "a5=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/Mass_array.npy')\n",
    "a6=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/Nodule_array.npy')\n",
    "a8=np.load('/content/drive/MyDrive/Colab Notebooks/4lab/data/npz&npy/256px,max/Pneumothorax_array.npy')\n",
    "x=np.concatenate([a,a1,a2,a3,a4,a5,a6,a8],axis=0)\n",
    "No_Finding_label=np.array([0]*len(a))\n",
    "Disease_label1=np.array([1]*len(a1))\n",
    "Disease_label2=np.array([2]*len(a2))\n",
    "Disease_label3=np.array([3]*len(a3))\n",
    "Disease_label4=np.array([4]*len(a4))\n",
    "Disease_label5=np.array([5]*len(a5))\n",
    "Disease_label6=np.array([6]*len(a6))\n",
    "Disease_label7=np.array([7]*len(a8))\n",
    "y=np.concatenate([No_Finding_label,Disease_label1,Disease_label2,Disease_label3,Disease_label4,Disease_label5,Disease_label6,Disease_label7],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "#画像データおよびラベルの 8 割を訓練用、2 割をテスト用に分ける\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20,random_state=0)\n",
    "#画像データの正規化を行う\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "\n",
    "# PyTorchで扱うため、tensor型にする\n",
    "x_train= torch.tensor(x_train)\n",
    "x_test= torch.tensor(x_test)\n",
    "y_train= torch.tensor(y_train)\n",
    "y_test= torch.tensor(y_test)\n",
    "#NCWH形式にする\n",
    "x_train= x_train.permute(0,3,1,2)\n",
    "x_test= x_test.permute(0,3,1,2)\n",
    "\n",
    "# 画像データ・正解ラベルのペアをデータにセットする\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "# セットしたデータをバッチサイズごとの配列に入れる。\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)#バッチサイズでメモリ消費変わる\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DualNetの作成(DualVGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Block1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Block1, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "class Block2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Block2, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DualVGG16(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DualVGG16, self).__init__()\n",
    "        self.s1_extractor = self._make_extractor()\n",
    "        self.s2_extractor = self._make_extractor()\n",
    "        self.fused_classifier = self._make_classifier()\n",
    "        self.s1_classifier = self._make_classifier()\n",
    "        self.s2_classifier = self._make_classifier()\n",
    "\n",
    "    def _make_extractor(self):\n",
    "        extractor = nn.Sequential(\n",
    "            Block1(1, 64),\n",
    "            Block1(64, 128),\n",
    "            Block2(128, 256),\n",
    "            Block2(256, 512),\n",
    "            Block2(512, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        return extractor\n",
    "\n",
    "    def _make_classifier(self):\n",
    "        return Classifier(512*8*8, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1_features = self.s1_extractor(x)\n",
    "        s2_features = self.s2_extractor(x)\n",
    "        fused_features = s1_features + s2_features\n",
    "\n",
    "        fused_output = self.fused_classifier(fused_features)\n",
    "        s1_output = self.s1_classifier(s1_features)\n",
    "        s2_output = self.s2_classifier(s2_features)\n",
    "        return fused_output, s1_output, s2_output\n",
    "\n",
    "    def check_cnn_size(self, size_check):\n",
    "        out = self.conv_layers(size_check)\n",
    "\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "num_classes = 8\n",
    "model = DualVGG16(num_classes)\n",
    "device = torch.device(\"cuda\")\n",
    "# modelはnn.Moduleを継承したクラス\n",
    "model = model.to(device) # GPUへ転送"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tensorflow.python.ops.gen_math_ops import TV_NotEqual_T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "'''評価用の関数を定義'''\n",
    "def valid(test_loader):\n",
    "\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            fused_output, s1_output, s2_output = model(images)\n",
    "            outputs=fused_output+0.3*s1_output+0.3*s2_output\n",
    "            outputs=F.softmax(outputs,dim=1)\n",
    "            loss_fused = criterion(fused_output, labels)\n",
    "            loss_s1 = criterion(s1_output, labels)\n",
    "            loss_s2 = criterion(s2_output, labels)\n",
    "            total_loss = loss_fused + 0.3*loss_s1 + 0.3*loss_s2\n",
    "            running_loss += total_loss.item()\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            labels = labels.view_as(predicted)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss = running_loss / len(test_loader)\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_loss_value=[]       #trainのlossを保持するlist\n",
    "train_acc_value=[]        #trainのaccuracyを保持するlist\n",
    "test_loss_value=[]       #testのlossを保持するlist\n",
    "test_acc_value=[]        #testのaccuracyを保持するlist\n",
    "# 学習\n",
    "num_epochs =15\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_batches=0\n",
    "    model.train()\n",
    "    if epoch%2==0:\n",
    "      # パラメータ固定\n",
    "      for param in model.s1_extractor.parameters():\n",
    "        param.required_grad=False\n",
    "      for param in model.s1_classifier.parameters():\n",
    "        param.required_grad=False\n",
    "\n",
    "      for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()# 勾配を初期化\n",
    "        #output\n",
    "        fused_output, s1_output, s2_output = model(images) # 順伝播\n",
    "        outputs=fused_output+0.3*s1_output+0.3*s2_output\n",
    "        outputs=F.softmax(outputs,dim=1)\n",
    "        #loss\n",
    "        loss_fused = criterion(fused_output, labels)\n",
    "        loss_s2 = criterion(s2_output, labels)\n",
    "        total_loss = loss_fused + 0.3*loss_s2\n",
    "        train_batches+=1\n",
    "        #結果を保存\n",
    "        train_loss += total_loss.item()\n",
    "        train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "        #逆伝搬\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "      #パラメータを戻す\n",
    "      for param in model.s1_extractor.parameters():\n",
    "        param.required_grad=True\n",
    "      for param in model.s1_classifier.parameters():\n",
    "        param.required_grad=True\n",
    "    else :\n",
    "      # パラメータ固定\n",
    "      for param in model.s2_extractor.parameters():\n",
    "        param.required_grad=False\n",
    "      for param in model.s2_classifier.parameters():\n",
    "        param.required_grad=False\n",
    "\n",
    "      for images, labels in train_loader:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          optimizer.zero_grad()# 勾配を初期化\n",
    "          #output\n",
    "          fused_output, s1_output, s2_output = model(images) # 順伝播\n",
    "          outputs=fused_output+0.3*s1_output+0.3*s2_output\n",
    "          outputs=F.softmax(outputs,dim=1)\n",
    "          #loss\n",
    "          loss_fused = criterion(fused_output, labels)\n",
    "          loss_s1 = criterion(s1_output, labels)\n",
    "          total_loss = loss_fused + 0.3*loss_s1\n",
    "          train_batches+=1\n",
    "          #結果を保存\n",
    "          train_loss += total_loss.item()\n",
    "          train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "          #逆伝搬\n",
    "          total_loss.backward()\n",
    "          optimizer.step()\n",
    "      #パラメータを戻す\n",
    "      for param in model.s2_extractor.parameters():\n",
    "        param.required_grad=True\n",
    "      for param in model.s2_classifier.parameters():\n",
    "        param.required_grad=True\n",
    "    #訓練結果の保存\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    avg_train_acc = train_acc / len(train_loader.dataset)\n",
    "    train_loss_value.append(avg_train_loss)\n",
    "    train_acc_value.append(avg_train_acc)\n",
    "    #テスト\n",
    "    model.eval()\n",
    "    avg_val_loss,avg_val_acc=valid(test_loader)\n",
    "    #テスト結果の保存\n",
    "    test_loss_value.append(avg_val_loss)\n",
    "    test_acc_value.append(avg_val_acc)\n",
    "    end = time.time()\n",
    "    # ログの表示\n",
    "    print('Epoch [{}/{}], train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f},time: {time:.4f}'\n",
    "                   .format(epoch+1, num_epochs, train_loss=avg_train_loss,train_acc=avg_train_acc, val_loss=avg_val_loss, val_acc=avg_val_acc,time=end-start))\n",
    "\n",
    "# モデルを保存する。\n",
    "#torch.save(model, \"/content/drive/MyDrive/Colab Notebooks/4lab/code/model/Dual_VGG16(5000-2).pth\")\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/4lab/code/model/DualNet_VGG16_weights(3000).pth')\n",
    "plt.figure(figsize=(6,6))      #グラフ描画用\n",
    "#以下グラフ描画\n",
    "plt.plot(range(1,num_epochs+1), train_loss_value)\n",
    "plt.plot(range(1,num_epochs+1), test_loss_value, c='#00ff00')\n",
    "plt.xlim(0, num_epochs+1)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('LOSS')\n",
    "plt.legend(['train loss','test loss'])\n",
    "plt.title('loss')\n",
    "plt.plot()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(range(1,num_epochs+1), train_acc_value)\n",
    "plt.plot(range(1,num_epochs+1), test_acc_value, c='#00ff00')\n",
    "plt.xlim(0, num_epochs+1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('ACCURACY')\n",
    "plt.legend( ['train acc','test acc'])\n",
    "plt.title('accuracy')\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC Scoreの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "model.load_state_dict(torch.load( '/content/drive/MyDrive/Colab Notebooks/4lab/code/model/DualNet_VGG16_weights(3000).pth'))\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        fused_output, s1_output, s2_output = model(inputs)\n",
    "        outputs = fused_output+0.3*s1_output+0.3*s2_output\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "Disease_label=[0,1,2,3,4,5,6,7]\n",
    "Disease_name=['No_Finding','Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule','Pneumothorax']\n",
    "actuals_one_hot = label_binarize(actuals, classes=Disease_label)\n",
    "predictions = np.array(predictions)　\n",
    "predictions = softmax(predictions)\n",
    "\n",
    "\n",
    "\n",
    "n_classes = len(Disease_label)\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(actuals_one_hot[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=Disease_name[i])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "c=0\n",
    "for i in range(n_classes):\n",
    "    print(Disease_name[i],\":\",auc(fpr[i],tpr[i]))\n",
    "    c+=auc(fpr[i],tpr[i])\n",
    "print('overall',c/8)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# おまけ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DualVGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class Block1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Block1, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Block2, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DualVGG19(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DualVGG19, self).__init__()\n",
    "        self.s1_extractor = self._make_extractor()\n",
    "        self.s2_extractor = self._make_extractor()\n",
    "        self.fused_classifier = self._make_classifier()\n",
    "        self.s1_classifier = self._make_classifier()\n",
    "        self.s2_classifier = self._make_classifier()\n",
    "\n",
    "    def _make_extractor(self):\n",
    "        extractor = nn.Sequential(\n",
    "            Block1(1, 64),\n",
    "            Block1(64, 128),\n",
    "            Block2(128, 256),\n",
    "            Block2(256, 512),\n",
    "            Block2(512, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        return extractor\n",
    "\n",
    "    def _make_classifier(self):\n",
    "        return Classifier(512*8*8, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1_features = self.s1_extractor(x)\n",
    "        s2_features = self.s2_extractor(x)\n",
    "        fused_features = s1_features + s2_features\n",
    "\n",
    "        fused_output = self.fused_classifier(fused_features)\n",
    "        s1_output = self.s1_classifier(s1_features)\n",
    "        s2_output = self.s2_classifier(s2_features)\n",
    "        return fused_output, s1_output, s2_output\n",
    "\n",
    "    def check_cnn_size(self, size_check):\n",
    "        out = self.conv_layers(size_check)\n",
    "\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "num_classes = 8\n",
    "model = DualVGG19(num_classes)\n",
    "device = torch.device(\"cuda\")\n",
    "# modelはnn.Moduleを継承したクラス\n",
    "model = model.to(device) # GPUへ転送"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DualResNet(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "    )\n",
    "#bottleneckブロックは1×1,3x3、1x1の3つの畳み込み層から構成されている。\n",
    "#expansionは～\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4  # 出力のチャンネル数を入力のチャンネル数の何倍に拡大するか\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv1x1(in_channels, channels)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = conv3x3(channels, channels, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = conv1x1(channels, channels * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 入力と出力のチャンネル数が異なる場合、x をダウンサンプリングする。\n",
    "        if in_channels != channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                conv1x1(in_channels, channels * self.expansion, stride),\n",
    "                nn.BatchNorm2d(channels * self.expansion),\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc =nn.Linear(in_features*4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.s1_extractor = self._make_extractor(block,)\n",
    "        self.s2_extractor = self._make_extractor()\n",
    "        self.fused_classifier = self._make_classifier()\n",
    "        self.s1_classifier = self._make_classifier()\n",
    "        self.s2_classifier = self._make_classifier()\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # 重みを初期化する。（Heの初期化法）\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, channels, blocks, stride):\n",
    "        layers = []\n",
    "\n",
    "        # 最初の Residual Block\n",
    "        layers.append(block(self.in_channels, channels, stride))\n",
    "\n",
    "        # 残りの Residual Block\n",
    "        self.in_channels = channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_extractor(self,block,channels, blocks, stride):\n",
    "        self.in_channels = 64#self.conv1を通過した後のチャンネル\n",
    "        extractor=nn.Sequential(\n",
    "            conv1 = nn.Conv2d(\n",
    "            1, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False\n",
    "            ),\n",
    "            bn1 = nn.BatchNorm2d(self.in_channels),\n",
    "            relu = nn.ReLU(inplace=True),\n",
    "            maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            layer1 = self._make_layer(block, 64, layers[0], stride=1),\n",
    "            layer2 = self._make_layer(block, 128, layers[1], stride=2),\n",
    "            layer3 = self._make_layer(block, 256, layers[2], stride=2),\n",
    "            layer4 = self._make_layer(block, 512, layers[3], stride=2),\n",
    "            avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        return extractor\n",
    "    def _make_classifier(self):\n",
    "        return Classifier(512,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1_features = self.s1_extractor(x)\n",
    "        s2_features = self.s2_extractor(x)\n",
    "        fused_features = s1_features + s2_features\n",
    "        fused_output = self.fused_classifier(fused_features)\n",
    "        s1_output = self.s1_classifier(s1_features)\n",
    "        s2_output = self.s2_classifier(s2_features)\n",
    "        return fused_output, s1_output, s2_output\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "\n",
    "# モデルのインスタンス化\n",
    "num_classes = 8\n",
    "custom_model = resnet50()\n",
    "device = torch.device(\"cuda\")\n",
    "custom_model=custom_model.to(device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
